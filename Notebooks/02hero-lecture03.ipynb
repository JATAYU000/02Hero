{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:17.024784Z","iopub.execute_input":"2025-03-16T11:55:17.025123Z","iopub.status.idle":"2025-03-16T11:55:17.322094Z","shell.execute_reply.started":"2025-03-16T11:55:17.025078Z","shell.execute_reply":"2025-03-16T11:55:17.321174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n!git clone https://github.com/JATAYU000/02Hero","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:17.514626Z","iopub.execute_input":"2025-03-16T11:55:17.514964Z","iopub.status.idle":"2025-03-16T11:55:23.117583Z","shell.execute_reply.started":"2025-03-16T11:55:17.514943Z","shell.execute_reply":"2025-03-16T11:55:23.116682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"words = open('/kaggle/working/02Hero/names.txt', 'r').read().splitlines()\nwords[:8]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.119294Z","iopub.execute_input":"2025-03-16T11:55:23.120001Z","iopub.status.idle":"2025-03-16T11:55:23.130235Z","shell.execute_reply.started":"2025-03-16T11:55:23.119977Z","shell.execute_reply":"2025-03-16T11:55:23.129457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chars = sorted(list(set(''.join(words))))\nstr2int = {s:i+1 for i,s in enumerate(chars)}\nstr2int['.'] = 0\nint2str = {i:s for s,i in str2int.items()}\nprint(int2str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.131712Z","iopub.execute_input":"2025-03-16T11:55:23.131953Z","iopub.status.idle":"2025-03-16T11:55:23.151436Z","shell.execute_reply.started":"2025-03-16T11:55:23.131933Z","shell.execute_reply":"2025-03-16T11:55:23.150726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"block = 3\nX,Y = [] , []\n\nfor i in words[:5]:\n    print(i)\n    context = [0] * block\n    for ch in i +'.':\n        ix = str2int[ch]\n        X.append(context)\n        Y.append(ix)\n        print(context,' => ',int2str[ix])\n        context = context[1:] + [ix]\n\nX = torch.tensor(X)\nY = torch.tensor(Y)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.152750Z","iopub.execute_input":"2025-03-16T11:55:23.152971Z","iopub.status.idle":"2025-03-16T11:55:23.204354Z","shell.execute_reply.started":"2025-03-16T11:55:23.152953Z","shell.execute_reply":"2025-03-16T11:55:23.203739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# look up table cranking it down to 2 dim for now\nL = torch.randn((27,2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.205123Z","iopub.execute_input":"2025-03-16T11:55:23.205401Z","iopub.status.idle":"2025-03-16T11:55:23.221323Z","shell.execute_reply.started":"2025-03-16T11:55:23.205374Z","shell.execute_reply":"2025-03-16T11:55:23.220790Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# identical to L[5]\nF.one_hot(torch.tensor(5),num_classes=27).float() @ L","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.221922Z","iopub.execute_input":"2025-03-16T11:55:23.222111Z","iopub.status.idle":"2025-03-16T11:55:23.369787Z","shell.execute_reply.started":"2025-03-16T11:55:23.222094Z","shell.execute_reply":"2025-03-16T11:55:23.369153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# L[Xi]\nL[[5,6,7,7,7]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.370438Z","iopub.execute_input":"2025-03-16T11:55:23.370651Z","iopub.status.idle":"2025-03-16T11:55:23.378124Z","shell.execute_reply.started":"2025-03-16T11:55:23.370632Z","shell.execute_reply":"2025-03-16T11:55:23.377514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.379640Z","iopub.execute_input":"2025-03-16T11:55:23.379842Z","iopub.status.idle":"2025-03-16T11:55:23.394441Z","shell.execute_reply.started":"2025-03-16T11:55:23.379825Z","shell.execute_reply":"2025-03-16T11:55:23.393723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding = L[X]\nembedding.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.395453Z","iopub.execute_input":"2025-03-16T11:55:23.395799Z","iopub.status.idle":"2025-03-16T11:55:23.414400Z","shell.execute_reply.started":"2025-03-16T11:55:23.395771Z","shell.execute_reply":"2025-03-16T11:55:23.413831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"w1 = torch.randn(6,100)  # 6 inputs and 100 neurons in the first layer\nb1 = torch.randn(100)  # biases for the 100 neurons\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.415257Z","iopub.execute_input":"2025-03-16T11:55:23.415563Z","iopub.status.idle":"2025-03-16T11:55:23.431598Z","shell.execute_reply.started":"2025-03-16T11:55:23.415533Z","shell.execute_reply":"2025-03-16T11:55:23.431032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# embedding[:, 0, :] , embedding[:, 1, :] , embedding[:, 2, :] these are the 3 inputs \n# these are 32 x 2 i want to catenate not 0th but axis 1\ntorch.cat([embedding[:, 0, :] , embedding[:, 1, :] , embedding[:, 2, :]],1).shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.535513Z","iopub.execute_input":"2025-03-16T11:55:23.535761Z","iopub.status.idle":"2025-03-16T11:55:23.556363Z","shell.execute_reply.started":"2025-03-16T11:55:23.535741Z","shell.execute_reply":"2025-03-16T11:55:23.555628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# unbind give the list [embedding[:, 0, :] , embedding[:, 1, :] , embedding[:, 2, :]] since the block size might not be 3 all the time\ntorch.cat(torch.unbind(embedding,1),1).shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.729630Z","iopub.execute_input":"2025-03-16T11:55:23.729859Z","iopub.status.idle":"2025-03-16T11:55:23.736575Z","shell.execute_reply.started":"2025-03-16T11:55:23.729840Z","shell.execute_reply":"2025-03-16T11:55:23.735796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# efficient way in torch\nembedding.view(32,6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:23.924331Z","iopub.execute_input":"2025-03-16T11:55:23.924621Z","iopub.status.idle":"2025-03-16T11:55:23.931336Z","shell.execute_reply.started":"2025-03-16T11:55:23.924600Z","shell.execute_reply":"2025-03-16T11:55:23.930558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layer1 = torch.tanh(embedding.view(embedding.shape[0],6) @ w1 + b1)\nlayer1.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:24.119668Z","iopub.execute_input":"2025-03-16T11:55:24.119890Z","iopub.status.idle":"2025-03-16T11:55:24.154526Z","shell.execute_reply.started":"2025-03-16T11:55:24.119870Z","shell.execute_reply":"2025-03-16T11:55:24.153856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"w2 = torch.randn(100,27)  # 100 inputs and 27 neurons in the first layer\nb2 = torch.randn(27)  # biases for the 27 output neurons","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:24.299755Z","iopub.execute_input":"2025-03-16T11:55:24.299943Z","iopub.status.idle":"2025-03-16T11:55:24.303528Z","shell.execute_reply.started":"2025-03-16T11:55:24.299927Z","shell.execute_reply":"2025-03-16T11:55:24.302870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logits = layer1 @ w2 + b2\nlogits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:24.508935Z","iopub.execute_input":"2025-03-16T11:55:24.509139Z","iopub.status.idle":"2025-03-16T11:55:24.514289Z","shell.execute_reply.started":"2025-03-16T11:55:24.509122Z","shell.execute_reply":"2025-03-16T11:55:24.513303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fake_counts = logits.exp()\np = fake_counts / fake_counts.sum(1, keepdims=True)\np.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:26.905090Z","iopub.execute_input":"2025-03-16T11:55:26.905383Z","iopub.status.idle":"2025-03-16T11:55:26.918559Z","shell.execute_reply.started":"2025-03-16T11:55:26.905359Z","shell.execute_reply":"2025-03-16T11:55:26.917770Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# now get the prob of each row's correct label y\nprob = p[torch.arange(embedding.shape[0]),Y]\nprob.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:27.097418Z","iopub.execute_input":"2025-03-16T11:55:27.097685Z","iopub.status.idle":"2025-03-16T11:55:27.108021Z","shell.execute_reply.started":"2025-03-16T11:55:27.097664Z","shell.execute_reply":"2025-03-16T11:55:27.107224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"neg_log_like = - p[torch.arange(embedding.shape[0]),Y].log().mean()\nneg_log_like","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:27.259567Z","iopub.execute_input":"2025-03-16T11:55:27.259770Z","iopub.status.idle":"2025-03-16T11:55:27.266330Z","shell.execute_reply.started":"2025-03-16T11:55:27.259752Z","shell.execute_reply":"2025-03-16T11:55:27.265710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## ------------------------------------------------------------\nblock = 3\nX,Y = [] , []\nfor i in words:\n    context = [0] * block\n    for ch in i +'.':\n        ix = str2int[ch]\n        X.append(context)\n        Y.append(ix)\n        context = context[1:] + [ix]\n\nX = torch.tensor(X)\nY = torch.tensor(Y)\n\nX.shape,Y.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:30.240277Z","iopub.execute_input":"2025-03-16T11:55:30.240590Z","iopub.status.idle":"2025-03-16T11:55:30.800206Z","shell.execute_reply.started":"2025-03-16T11:55:30.240565Z","shell.execute_reply":"2025-03-16T11:55:30.799339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"L = torch.randn((27,2),requires_grad = True)\nw1 = torch.randn((6,100),requires_grad = True)  # 6 inputs and 100 neurons in the first layer\nb1 = torch.randn((100),requires_grad = True)  # biases for the 100 neurons\nw2 = torch.randn((100,27),requires_grad = True)  # 100 inputs and 27 neurons in the first layer\nb2 = torch.randn((27,),requires_grad = True) \nparams = [L,w1,b1,w2,b2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:30.801378Z","iopub.execute_input":"2025-03-16T11:55:30.801709Z","iopub.status.idle":"2025-03-16T11:55:30.807121Z","shell.execute_reply.started":"2025-03-16T11:55:30.801685Z","shell.execute_reply":"2025-03-16T11:55:30.806421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nlre = torch.linspace(-3,0,1000) # 10^-3 to 10^0\nlrs = 10**lre\n\nlri = []\nlossi = []\nfor i in range(1000):\n    # mini batch\n    rix = torch.randint(0,X.shape[0],(32,)) # 32 batch size\n    \n    # forward\n    emb = L[X[rix]] # (32,3,2)\n    layerone = torch.tanh(emb.view(emb.shape[0],6) @ w1 + b1) # (32,100)\n    logits = layerone @ w2 + b2 # (32, 27)\n    # counts = logits.exp()\n    # p = counts / counts.sum(1,keepdim=True)\n    # loss = - p[torch.arange(emb.shape[0]),Y].log().mean()\n    loss = F.cross_entropy(logits,Y[rix])  # replace the three lines efficiently\n    # print(loss.item())\n\n    # backward\n    for p in params:\n        p.grad = None\n    loss.backward()\n    \n    #update\n    lr = lrs[i] # decayed learning rate\n    for p in params:\n        p.data += -lr * p.grad\n\n    # track stats\n    lri.append(lre[i])\n    lossi.append(loss.item())\n\nprint(loss.item())\n\nplt.plot(lri,lossi)\n\n'''\n\nsum(p.nelement() for p in params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:32.269635Z","iopub.execute_input":"2025-03-16T11:55:32.269921Z","iopub.status.idle":"2025-03-16T11:55:32.275392Z","shell.execute_reply.started":"2025-03-16T11:55:32.269899Z","shell.execute_reply":"2025-03-16T11:55:32.274577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plt.plot(lri,lossi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:32.424151Z","iopub.execute_input":"2025-03-16T11:55:32.424355Z","iopub.status.idle":"2025-03-16T11:55:32.427637Z","shell.execute_reply.started":"2025-03-16T11:55:32.424338Z","shell.execute_reply":"2025-03-16T11:55:32.426811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10000):\n    # mini batch\n    rix = torch.randint(0,X.shape[0],(32,)) # 32 batch size\n    \n    # forward\n    emb = L[X[rix]] # (32,3,2)\n    layerone = torch.tanh(emb.view(emb.shape[0],6) @ w1 + b1) # (32,100)\n    logits = layerone @ w2 + b2 # (32, 27)\n    loss = F.cross_entropy(logits,Y[rix])  # replace the three lines efficiently\n\n    # backward\n    for p in params:\n        p.grad = None\n    loss.backward()\n    \n    #update\n    lr = 0.1  # a good learning rate found with the loss graph\n    for p in params:\n        p.data += -lr * p.grad\n\nprint(loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:35.120089Z","iopub.execute_input":"2025-03-16T11:55:35.120425Z","iopub.status.idle":"2025-03-16T11:55:41.011349Z","shell.execute_reply.started":"2025-03-16T11:55:35.120400Z","shell.execute_reply":"2025-03-16T11:55:41.010449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# loss on entire dataset\nemb = L[X] \nlayerone = torch.tanh(emb.view(emb.shape[0],6) @ w1 + b1) \nlogits = layerone @ w2 + b2 \nloss = F.cross_entropy(logits,Y) \nloss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:41.012579Z","iopub.execute_input":"2025-03-16T11:55:41.012839Z","iopub.status.idle":"2025-03-16T11:55:41.226210Z","shell.execute_reply.started":"2025-03-16T11:55:41.012818Z","shell.execute_reply":"2025-03-16T11:55:41.225284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train valid test split => 80% 10% 10%\ndef build_dataset(words):\n    block = 3\n    X,Y = [] , []\n    for i in words:\n        context = [0] * block\n        for ch in i +'.':\n            ix = str2int[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n    \n    X = torch.tensor(X, device=device)\n    Y = torch.tensor(Y, device=device)\n    \n    print(X.shape,Y.shape)\n    return X,Y\n\nimport random\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtrn,Ytrn = build_dataset(words[:n1])\nXval,Yval = build_dataset(words[n1:n2])\nXtest,Ytest = build_dataset(words[n2:])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:00:50.174454Z","iopub.execute_input":"2025-03-16T12:00:50.174785Z","iopub.status.idle":"2025-03-16T12:00:50.682025Z","shell.execute_reply.started":"2025-03-16T12:00:50.174762Z","shell.execute_reply":"2025-03-16T12:00:50.681080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"L = torch.randn((27,10),requires_grad = True)   # 10 dimensional embedding vectors\nw1 = torch.randn((30,200),requires_grad = True)  # 3x10 30 inputs and 300 neurons in the first layer\nb1 = torch.randn((200),requires_grad = True)  # biases for the 300 neurons\nw2 = torch.randn((200,27),requires_grad = True)  # 300 inputs and 27 neurons in the first layer\nb2 = torch.randn((27,),requires_grad = True) \nparams = [L,w1,b1,w2,b2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:41.806078Z","iopub.execute_input":"2025-03-16T11:55:41.806355Z","iopub.status.idle":"2025-03-16T11:55:41.811420Z","shell.execute_reply.started":"2025-03-16T11:55:41.806333Z","shell.execute_reply":"2025-03-16T11:55:41.810440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stepi=[]\nlossi=[]\nfor i in range(30000):\n    # mini batch\n    rix = torch.randint(0,Xtrn.shape[0],(32,)) # 32 batch size\n    \n    # forward\n    emb = L[Xtrn[rix]] # (32,3,2)\n    layerone = torch.tanh(emb.view(emb.shape[0],w1.shape[0]) @ w1 + b1) # (32,100)\n    logits = layerone @ w2 + b2 # (32, 27)\n    loss = F.cross_entropy(logits,Ytrn[rix])  # replace the three lines efficiently\n\n    # backward\n    for p in params:\n        p.grad = None\n    loss.backward()\n    \n    #update\n    lr = 0.01  # a good learning rate found with the loss graph\n    for p in params:\n        p.data += -lr * p.grad\n\n    # track\n    stepi.append(i)\n    lossi.append(loss.item())\n\nprint(loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:55:41.812194Z","iopub.execute_input":"2025-03-16T11:55:41.812520Z","iopub.status.idle":"2025-03-16T11:56:00.347525Z","shell.execute_reply.started":"2025-03-16T11:55:41.812491Z","shell.execute_reply":"2025-03-16T11:56:00.346575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(stepi,lossi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:56:00.348433Z","iopub.execute_input":"2025-03-16T11:56:00.348773Z","iopub.status.idle":"2025-03-16T11:56:00.681913Z","shell.execute_reply.started":"2025-03-16T11:56:00.348749Z","shell.execute_reply":"2025-03-16T11:56:00.681073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## train loss\nemb = L[Xtrn] \nlayerone = torch.tanh(emb.view(emb.shape[0],w1.shape[0]) @ w1 + b1) \nlogits = layerone @ w2 + b2 \nloss = F.cross_entropy(logits,Ytrn) \nloss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:56:00.682727Z","iopub.execute_input":"2025-03-16T11:56:00.682949Z","iopub.status.idle":"2025-03-16T11:56:00.982140Z","shell.execute_reply.started":"2025-03-16T11:56:00.682929Z","shell.execute_reply":"2025-03-16T11:56:00.981350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## validation\nemb = L[Xval] \nlayerone = torch.tanh(emb.view(emb.shape[0],w1.shape[0]) @ w1 + b1) \nlogits = layerone @ w2 + b2 \nloss = F.cross_entropy(logits,Yval) \nloss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:56:00.983652Z","iopub.execute_input":"2025-03-16T11:56:00.983895Z","iopub.status.idle":"2025-03-16T11:56:01.015739Z","shell.execute_reply.started":"2025-03-16T11:56:00.983874Z","shell.execute_reply":"2025-03-16T11:56:01.014871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndef reg(params):\n    sum = 0\n    for p in params:\n        sum += (p**2).mean()\n    sum /= len(params)\n    return sum\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T14:11:40.040962Z","iopub.execute_input":"2025-03-16T14:11:40.041270Z","iopub.status.idle":"2025-03-16T14:11:40.046349Z","shell.execute_reply.started":"2025-03-16T14:11:40.041249Z","shell.execute_reply":"2025-03-16T14:11:40.045656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## trying to reduce loss\n\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtrn,Ytrn = build_dataset(words[:n1])\nXval,Yval = build_dataset(words[n1:n2])\nXtest,Ytest = build_dataset(words[n2:])\n\nbatch = 32\nneurons = 200\nembed_size = 40\nblock = 3\n\nL = torch.randn((27,embed_size),requires_grad = True, device=device)   \nw1 = torch.randn((embed_size*block,neurons),requires_grad = True, device=device)  \nb1 = torch.randn((neurons),requires_grad = True, device=device)  \nw2 = torch.randn((neurons,27),requires_grad = True, device=device)  \nb2 = torch.randn((27,),requires_grad = True, device=device) \nparams = [L,w1,b1,w2,b2]\n\nprint(f\"\\nembedding size : {L.shape[1]} \\nneurons: {w1.shape[1]} \\nbatch size: {batch}\")\n\nstepi=[]\nlossi=[]\nfor i in range(200000):\n    \n    # mini batch\n    rix = torch.randint(0,Xtrn.shape[0],(batch,)) \n    \n    # forward\n    emb = L[Xtrn[rix]] \n    layerone = torch.tanh(emb.view(emb.shape[0],w1.shape[0]) @ w1 + b1) \n    logits = layerone @ w2 + b2 \n    loss = F.cross_entropy(logits,Ytrn[rix]) + 0.005 * reg(params)\n\n    # backward\n    for p in params:\n        p.grad = None\n    loss.backward()\n    \n    #update\n    lr = 0.1\n    if i>60000:lr=0.07\n    if i>120000:lr = 0.01\n        \n    for p in params:\n        p.data += -lr * p.grad\n\n    # track\n    stepi.append(i)\n    lossi.append(loss.item())\n\n\n## train loss\nemb = L[Xtrn] \nlayerone = torch.tanh(emb.view(emb.shape[0],w1.shape[0]) @ w1 + b1) \nlogits = layerone @ w2 + b2\nloss = F.cross_entropy(logits,Ytrn)\nprint(f\"training loss: {loss.item():.4f}\")\n\n## validation\nemb = L[Xval] \nlayerone = torch.tanh(emb.view(emb.shape[0],w1.shape[0]) @ w1 + b1) \nlogits = layerone @ w2 + b2 \nloss = F.cross_entropy(logits,Yval) \nprint(f\"Validation loss: {loss.item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T14:18:50.204767Z","iopub.execute_input":"2025-03-16T14:18:50.205076Z","iopub.status.idle":"2025-03-16T14:25:13.918904Z","shell.execute_reply.started":"2025-03-16T14:18:50.205052Z","shell.execute_reply":"2025-03-16T14:25:13.918054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## observations\nplt.plot(stepi,lossi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:45:22.827897Z","iopub.execute_input":"2025-03-16T11:45:22.828251Z","iopub.status.idle":"2025-03-16T11:45:23.083512Z","shell.execute_reply.started":"2025-03-16T11:45:22.828226Z","shell.execute_reply":"2025-03-16T11:45:23.082407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emb = L[Xtest] \nlayerone = torch.tanh(emb.view(emb.shape[0],w1.shape[0]) @ w1 + b1) \nlogits = layerone @ w2 + b2\nloss = F.cross_entropy(logits,Ytest)\nprint(f\"test loss: {loss.item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T14:26:23.667186Z","iopub.execute_input":"2025-03-16T14:26:23.667532Z","iopub.status.idle":"2025-03-16T14:26:23.674455Z","shell.execute_reply.started":"2025-03-16T14:26:23.667501Z","shell.execute_reply":"2025-03-16T14:26:23.673698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}